{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Tools\n",
        "1. NumPy\n",
        "2. MatplotLib"
      ],
      "metadata": {
        "id": "Km7WTeoUBvTA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "np.set_printoptions(precision=2)  # reduced display precision on numpy arrays"
      ],
      "metadata": {
        "id": "SHqDw3nzBtuo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Problem Statement\n",
        "\n",
        "We will use the example of housing price prediction. The training dataset contains three examples with four features (size, bedrooms, floors and, age) shown in the table below.  Note that, unlike the earlier labs, size is in sqft rather than 1000 sqft. This causes an issue, which you will solve in the next lab!\n",
        "\n",
        "| Size (sqft) | Number of Bedrooms  | Number of floors | Age of  Home | Price (1000s dollars)  |   \n",
        "| ----------------| ------------------- |----------------- |--------------|-------------- |  \n",
        "| 2104            | 5                   | 1                | 45           | 460           |  \n",
        "| 1416            | 3                   | 2                | 40           | 232           |  \n",
        "| 852             | 2                   | 1                | 35           | 178           |  \n",
        "\n",
        "We will build a linear regression model using these values so you can then predict the price for other houses. For example, a house with 1200 sqft, 3 bedrooms, 1 floor, 40 years old.  "
      ],
      "metadata": {
        "id": "zOm-hGcvBXz3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training Data (multiple features)\n",
        "x_train = np.array([[2104, 5, 1, 45], \n",
        "                    [1416, 3, 2, 40], \n",
        "                    [852, 2, 1, 35]])\n",
        "# Target data\n",
        "y_train = np.array([460, 232, 178])"
      ],
      "metadata": {
        "id": "vB2nFRJZBeV0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Matrix X containing our Training Data\n",
        "Similar to the table above, examples are stored in a NumPy matrix `X_train`. Each row of the matrix represents one example. When you have $m$ training examples ( $m$ is 3 in our example), and there are $n$ features (4 in our example), $\\mathbf{X}$ is a matrix with dimensions ($m$, $n$) (m rows, n columns).\n",
        "\n",
        "\n",
        "$$\\mathbf{X} = \n",
        "\\begin{pmatrix}\n",
        " x^{(0)}_0 & x^{(0)}_1 & \\cdots & x^{(0)}_{n-1} \\\\ \n",
        " x^{(1)}_0 & x^{(1)}_1 & \\cdots & x^{(1)}_{n-1} \\\\\n",
        " \\cdots \\\\\n",
        " x^{(m-1)}_0 & x^{(m-1)}_1 & \\cdots & x^{(m-1)}_{n-1} \n",
        "\\end{pmatrix}\n",
        "$$\n",
        "notation:\n",
        "- $\\mathbf{x}^{(i)}$ is vector containing example i. $\\mathbf{x}^{(i)}$ $ = (x^{(i)}_0, x^{(i)}_1, \\cdots,x^{(i)}_{n-1})$\n",
        "- $x^{(i)}_j$ is element j in example i. The superscript in parenthesis indicates the example number while the subscript represents an element.  "
      ],
      "metadata": {
        "id": "oO3KcVRTCYng"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"X-Shape = {x_train.shape}\")\n",
        "print(f\"Y-Shape = {y_train.shape}\")\n",
        "print(f\"X-train = \\n{x_train}\")\n",
        "print(f\"Y-train = {y_train}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XSQOh05tCjr6",
        "outputId": "8dad9102-4852-409b-9fc6-f01bba5d4018"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X-Shape = (3, 4)\n",
            "Y-Shape = (3,)\n",
            "X-train = \n",
            "[[2104    5    1   45]\n",
            " [1416    3    2   40]\n",
            " [ 852    2    1   35]]\n",
            "Y-train = [460 232 178]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Parameter vector w, b\n",
        "\n",
        "* $\\mathbf{w}$ is a vector with $n$ elements.\n",
        "  - Each element contains the parameter associated with one feature.\n",
        "  - in our dataset, n is 4.\n",
        "  - notionally, we draw this as a column vector\n",
        "\n",
        "$$\\mathbf{w} = \\begin{pmatrix}\n",
        "w_0 \\\\ \n",
        "w_1 \\\\\n",
        "\\cdots\\\\\n",
        "w_{n-1}\n",
        "\\end{pmatrix}\n",
        "$$\n",
        "* $b$ is a scalar parameter."
      ],
      "metadata": {
        "id": "wdZgc-KEDhra"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# For demonstration, w and b are be loaded with some initial selected values that are near the optimal. Also, w is a 1-D NumPy vector.\n",
        "w_init = np.array([ 0.39133535, 18.75376741, -53.36032453, -26.42131618])\n",
        "b_init = 785.1811367994083\n",
        "print(f\"w_init shape: {w_init.shape}, w_init type: {type(w_init)}\")\n",
        "print(f\"b_init type: {type(b_init)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aMVi8hIfDnt3",
        "outputId": "5149a7fb-6526-4991-f444-52e894d87f55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "w_init shape: (4,), w_init type: <class 'numpy.ndarray'>\n",
            "b_init type: <class 'float'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Model Prediction With Multiple Variables\n",
        "The model's prediction with multiple variables is given by the linear model:\n",
        "\n",
        "$$ f_{\\mathbf{w},b}(\\mathbf{x}) =  w_0x_0 + w_1x_1 +... + w_{n-1}x_{n-1} + b \\tag{1}$$\n",
        "or in vector notation:\n",
        "$$ f_{\\mathbf{w},b}(\\mathbf{x}) = \\mathbf{w} \\cdot \\mathbf{x} + b  \\tag{2} $$ \n",
        "where $\\cdot$ is a vector `dot product`\n",
        "\n",
        "To demonstrate the dot product, we will implement prediction using (1) and (2)."
      ],
      "metadata": {
        "id": "WW6m7xR_EWvG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Computes the model using the 'dot' method of the NumPy library using the above equation (2)\n",
        "# NOTE: It is computed for ONE Row, for instance, here we test it on the 0th row using 'x_train[0,:]'\n",
        "def compute_model(x, w, b):\n",
        "  \"\"\"\n",
        "      Args:\n",
        "      x (ndarray): Shape (n,) example with multiple features\n",
        "      w (ndarray): Shape (n,) model parameters   \n",
        "      b (scalar):             model parameter \n",
        "\n",
        "      Returns:\n",
        "      f_wb (scalar):  prediction\n",
        "  \"\"\"\n",
        "  f_wb = np.dot(w, x) + b\n",
        "  return f_wb\n",
        "\n",
        "# Testing the method by getting a 'Row' from the matrix\n",
        "print(f\"f_wb for 0th row (i=0) = {compute_model(x_train[0,:], w_init, b_init)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sNPkTaIEEh__",
        "outputId": "df3b67ca-c0a1-43a7-fe4c-4db72fac4b88"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "f_wb for 0th row (i=0) = 459.9999976194083\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2 Compute Cost With Multiple Variables\n",
        "The equation for the cost function with multiple variables $J(\\mathbf{w},b)$ is:\n",
        "$$J(\\mathbf{w},b) = \\frac{1}{2m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)})^2 \\tag{3}$$ \n",
        "where:\n",
        "$$ f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) = \\mathbf{w} \\cdot \\mathbf{x}^{(i)} + b  \\tag{4} $$ \n",
        "\n",
        "\n",
        "Here, $\\mathbf{w}$ and $\\mathbf{x}^{(i)}$ are vectors rather than scalars supporting multiple features."
      ],
      "metadata": {
        "id": "ZrU0Tdaah8Nd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Computes the cost J(w,b) for the training-data\n",
        "def compute_cost(X, y, w, b):\n",
        "  \"\"\"\n",
        "    Args:\n",
        "      X (ndarray (m,n)): Data, m examples with n features [Matrix]\n",
        "      y (ndarray (m,)) : target values\n",
        "      w (ndarray (n,)) : model parameters  \n",
        "      b (scalar)       : model parameter\n",
        "      \n",
        "    Returns:\n",
        "      cost (scalar): cost\n",
        "  \"\"\"\n",
        "  m = X.shape[0]                                # Row count\n",
        "  cost = 0\n",
        "  for i in range (0, m):\n",
        "    f_wb = compute_model(X[i], w, b)            # Computing f_wb (prediction) for each row\n",
        "    cost = cost + (f_wb - y[i]) ** 2\n",
        "  total_cost = cost/(2*m)\n",
        "  return total_cost\n",
        "      \n",
        "# Testing the method\n",
        "print(f\"Total Cost = {compute_cost(x_train, y_train, w_init, b_init)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J4feyFQ1h95x",
        "outputId": "b97c7a4b-abf6-45fc-8c93-c9b17cfdafbb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Cost = 1.5578904428966628e-12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3 Gradient Descent With Multiple Variables\n",
        "Gradient descent for multiple variables:\n",
        "\n",
        "$$\\begin{align*} \\text{repeat}&\\text{ until convergence:} \\; \\lbrace \\newline\\;\n",
        "& w_j = w_j -  \\alpha \\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j} \\tag{5}  \\; & \\text{for j = 0..n-1}\\newline\n",
        "&b\\ \\ = b -  \\alpha \\frac{\\partial J(\\mathbf{w},b)}{\\partial b}  \\newline \\rbrace\n",
        "\\end{align*}$$\n",
        "\n",
        "where, n is the number of features, parameters $w_j$,  $b$, are updated simultaneously and where  \n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "\\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)})x_{j}^{(i)} \\tag{6}  \\\\\n",
        "\\frac{\\partial J(\\mathbf{w},b)}{\\partial b}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)}) \\tag{7}\n",
        "\\end{align}\n",
        "$$\n",
        "* m is the number of training examples in the data set\n",
        "\n",
        "    \n",
        "*  $f_{\\mathbf{w},b}(\\mathbf{x}^{(i)})$ is the model's prediction, while $y^{(i)}$ is the target value\n"
      ],
      "metadata": {
        "id": "1wTn5XAlY1Yt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Computing the gradient (i.e) the derivatives for 'w' and 'b' using the equations (6) and (7) \n",
        "def compute_gradient(X, y, w, b):\n",
        "  \"\"\"\n",
        "    Computes the gradient for linear regression \n",
        "    Args:\n",
        "      X (ndarray (m,n)): Data, m examples with n features\n",
        "      y (ndarray (m,)) : target values\n",
        "      w (ndarray (n,)) : model parameters  \n",
        "      b (scalar)       : model parameter\n",
        "      \n",
        "    Returns:\n",
        "      dj_dw (ndarray (n,)): The gradient of the cost w.r.t. the parameters w. \n",
        "      dj_db (scalar):       The gradient of the cost w.r.t. the parameter b. \n",
        "    \"\"\"\n",
        "  m, n = X.shape\n",
        "  dj_dw = np.zeros((n,))\n",
        "  dj_db = 0.\n",
        "\n",
        "  for i in range (0, m):\n",
        "    f_wb = compute_model(X[i], w, b)\n",
        "    error = f_wb - y[i]\n",
        "\n",
        "    for j in range (0, n):\n",
        "      dj_dw[j] =  dj_dw[j] + error * X[i,j]\n",
        "    dj_db = dj_db + error\n",
        "\n",
        "  dj_dw = dj_dw/m\n",
        "  dj_db = dj_db/m\n",
        "  return dj_dw, dj_db\n",
        "      \n",
        "# Testing the method by computing and displaying gradient \n",
        "tmp_dj_db, tmp_dj_dw = compute_gradient(x_train, y_train, w_init, b_init)\n",
        "print(f'dj_db at initial w,b: {tmp_dj_db}')\n",
        "print(f'dj_dw at initial w,b: \\n {tmp_dj_dw}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "snXggo4MZUn4",
        "outputId": "913e2c59-24a8-4b46-b37a-e5b7f84047ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dj_db at initial w,b: [-2.73e-03 -6.27e-06 -2.22e-06 -6.92e-05]\n",
            "dj_dw at initial w,b: \n",
            " -1.6739251501955248e-06\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4 Gradient Descent With Multiple Variables\n",
        "The function below implements equation (5) above (i.e)\n",
        "\n",
        "$$\\begin{align*} \\text{repeat}&\\text{ until convergence:} \\; \\lbrace \\newline\\;\n",
        "& w_j = w_j -  \\alpha \\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j} \\tag{5}  \\; & \\text{for j = 0..n-1}\\newline\n",
        "&b\\ \\ = b -  \\alpha \\frac{\\partial J(\\mathbf{w},b)}{\\partial b}  \\newline \\rbrace\n",
        "\\end{align*}$$\n",
        "\n",
        "where, n is the number of features, parameters $w_j$,  $b$, are updated simultaneously and where  "
      ],
      "metadata": {
        "id": "aKKz0cxA6MgU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Frzy6WDu6MGF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient_descent(X, y, w, b, alpha, num_iters):\n",
        "  \"\"\"\n",
        "    Performs batch gradient descent to learn theta. Updates theta by taking \n",
        "    num_iters gradient steps with learning rate alpha\n",
        "    \n",
        "    Args:\n",
        "      X (ndarray (m,n))   : Data, m examples with n features\n",
        "      y (ndarray (m,))    : target values\n",
        "      w (ndarray (n,))    : initial model parameters  \n",
        "      b (scalar)          : initial model parameter\n",
        "      alpha (float)       : Learning rate\n",
        "      num_iters (int)     : number of iterations to run gradient descent\n",
        "      \n",
        "    Returns:\n",
        "      w (ndarray (n,))    : Updated values of parameters \n",
        "      b (scalar)          : Updated value of parameter \n",
        "    \"\"\"\n",
        "  # Important to assign w and b to local-variables to avoid modification of global w and b\n",
        "  w_out = w\n",
        "  b_out = b\n",
        "  for i in range (0, num_iters):\n",
        "    dj_w, dj_b = compute_gradient(X, y, w_out, b_out)\n",
        "      \n",
        "    w_out = w_out - (alpha * dj_w)\n",
        "    b_out = b_out - (alpha * dj_b)\n",
        "\n",
        "  return w_out, b_out\n",
        "\n",
        "\n",
        "# initialize parameters\n",
        "initial_w = np.zeros_like(w_init)\n",
        "initial_b = 0.\n",
        "# some gradient descent settings\n",
        "iterations = 1000\n",
        "alpha = 5.0e-7\n",
        "# run gradient descent \n",
        "w_final, b_final = gradient_descent(x_train, y_train, initial_w, initial_b, alpha, iterations)\n",
        "print(f\"b,w found by gradient descent: {b_final:0.2f},{w_final} \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IgiVEr9n40Vz",
        "outputId": "b047d0e0-afb2-436e-d6aa-cd3255e5ff9b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "b,w found by gradient descent: -0.00,[ 0.2   0.   -0.01 -0.07] \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Prediction\n",
        "m = x_train.shape[0]\n",
        "for i in range(m):\n",
        "    print(f\"prediction: {np.dot(x_train[i], w_final) + b_final:0.2f}, target value: {y_train[i]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "84IJWIpv-55S",
        "outputId": "cf18a44c-6c31-4f28-aa2d-7eaa53c15fde"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prediction: 426.19, target value: 460\n",
            "prediction: 286.17, target value: 232\n",
            "prediction: 171.47, target value: 178\n"
          ]
        }
      ]
    }
  ]
}